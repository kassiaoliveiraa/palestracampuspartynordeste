{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkLeheXzYlBkz4AZfymAA2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kassiaoliveiraa/palestracampuspartynordeste/blob/main/palestracampusparty.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZluslKEa1-Mv"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install pyarrow\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ETL\").getOrCreate()"
      ],
      "metadata": {
        "id": "9ps226HKpiFS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/home/etl/processo_inicio_1990.csv\"\n",
        "file_size_bytes = os.path.getsize(file_path)\n",
        "file_size_kb = file_size_bytes / 1024  # Converte de bytes para kilobytes\n",
        "\n",
        "print(f\"O tamanho do arquivo é {file_size_kb:.2f} KB\")"
      ],
      "metadata": {
        "id": "xI1Rv9FHpmwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .option(\"sep\", \";\") \\\n",
        "    .load(\"/home/etl/processo_inicio_1990.csv\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "BLiyjoWbqoR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"processos\")\n",
        "\n",
        "queryCount = \"\"\"\n",
        "SELECT count(*) AS count\n",
        "FROM processos\n",
        "\"\"\"\n",
        "\n",
        "queryprocAno = \"\"\"\n",
        "SELECT count(*) AS count, DTABERTURA\n",
        "FROM processos\n",
        "GROUP BY DTABERTURA\n",
        "\"\"\"\n",
        "\n",
        "result = spark.sql(queryCount)\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "id": "aX52vcf2rLNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.parquet(\"/home/etl/processos_temp/\")"
      ],
      "metadata": {
        "id": "PE16OuohrlAD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_folder = '/home/etl/processos_temp/'\n",
        "destination_folder = '/home/etl/procesosparquet/'\n",
        "\n",
        "os.makedirs(destination_folder, exist_ok=True)\n",
        "\n",
        "file_counter = 1\n",
        "\n",
        "for file_name in os.listdir(temp_folder):\n",
        "    if file_name.endswith(\".parquet\"):\n",
        "        source_file = os.path.join(temp_folder, file_name)\n",
        "\n",
        "        destination_file = os.path.join(destination_folder, f\"processos_{file_counter}.parquet\")\n",
        "\n",
        "        try:\n",
        "            shutil.move(source_file, destination_file)\n",
        "            file_counter += 1\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Erro ao mover o arquivo {source_file}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Erro inesperado: {e}\")\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(temp_folder)\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Erro ao remover a pasta temporária: {e}\")\n",
        "\n",
        "print(\"Todos os arquivos foram salvos com sucesso e renomeados.\")"
      ],
      "metadata": {
        "id": "zwMG5U-mw9Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read \\\n",
        "    .format(\"parquet\") \\\n",
        "    .load(\"/home/etl/procesosparquet\")\n",
        "\n",
        "df.count()"
      ],
      "metadata": {
        "id": "8NPDduLrs7nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_size_of_folder_in_kb(folder_path):\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(\".parquet\"):\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                total_size += os.path.getsize(file_path)\n",
        "    return total_size / 1024\n",
        "\n",
        "folder_path = '/home/etl/procesosparquet'\n",
        "total_size_kb = get_size_of_folder_in_kb(folder_path)\n",
        "\n",
        "print(f\"Tamanho total dos arquivos Parquet: {total_size_kb:.2f} KB\")"
      ],
      "metadata": {
        "id": "DHWKCuJC2Wiy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}